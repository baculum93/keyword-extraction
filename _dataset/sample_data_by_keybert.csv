preprocessed_title,preprocessed_abstract,kwrd_ttl_dflt_1,kwrd_ttl_dflt_2,kwrd_ttl_dflt_3,kwrd_abs_dflt_1,kwrd_abs_dflt_2,kwrd_abs_dflt_3,kwrd_all_dflt_1,kwrd_all_dflt_2,kwrd_all_dflt_3,kwrd_ttl_kcv,kwrd_abs_kcv,kwrd_all_kcv
Semi-Supervised Classification with Graph Convolutional Networks,We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.,"[('supervised', 0.4809), ('networks', 0.4462), ('graph', 0.4447), ('classification', 0.4094), ('convolutional', 0.3725)]","[('graph convolutional', 0.7242), ('semi supervised', 0.6387), ('convolutional networks', 0.5484), ('supervised classification', 0.4825), ('supervised', 0.4809)]","[('graph convolutional networks', 0.7825), ('graph convolutional', 0.7242), ('with graph convolutional', 0.7109), ('classification with graph', 0.6973), ('semi supervised classification', 0.6407)]","[('graphs', 0.4307), ('networks', 0.3926), ('graph', 0.3872), ('nodes', 0.3417), ('supervised', 0.3212)]","[('graph convolutions', 0.5941), ('graph dataset', 0.5351), ('citation networks', 0.5243), ('knowledge graph', 0.4937), ('spectral graph', 0.4919)]","[('learning on graph', 0.6207), ('graph convolutions', 0.5941), ('spectral graph convolutions', 0.5853), ('graph convolutions our', 0.5609), ('graph dataset', 0.5351)]","[('graphs', 0.4303), ('networks', 0.3887), ('graph', 0.3841), ('supervised', 0.3333), ('nodes', 0.3255)]","[('graph convolutional', 0.6346), ('graph convolutions', 0.6214), ('graph dataset', 0.5395), ('spectral graph', 0.5053), ('citation networks', 0.5005)]","[('graph convolutional networks', 0.6758), ('graph convolutional', 0.6346), ('graph convolutions', 0.6214), ('spectral graph convolutions', 0.6205), ('with graph convolutional', 0.6176)]","[('graph convolutional networks', 0.7825)]","[('spectral graph convolutions', 0.5853), ('citation networks', 0.5243), ('knowledge graph dataset', 0.5195), ('local graph structure', 0.4675), ('graphs', 0.4307)]","[('graph convolutional networks', 0.6758), ('spectral graph convolutions', 0.6205), ('citation networks', 0.5005), ('knowledge graph dataset', 0.4947), ('local graph structure', 0.4499)]"
Attention Is All You Need,"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.","[('attention', 0.8273), ('need', 0.3114), ('all', 0.2573), ('is', 0.2268), ('you', 0.1931)]","[('attention is', 0.8596), ('attention', 0.8273), ('need', 0.3114), ('is all', 0.2931), ('you need', 0.2777)]","[('attention is all', 0.8924), ('attention is', 0.8596), ('attention', 0.8273), ('all you need', 0.4574), ('need', 0.3114)]","[('attention', 0.3944), ('decoder', 0.3826), ('encoder', 0.3672), ('neural', 0.349), ('parsing', 0.3443)]","[('translation tasks', 0.5586), ('machine translation', 0.5223), ('translation task', 0.4846), ('constituency parsing', 0.3948), ('attention', 0.3944)]","[('machine translation tasks', 0.6133), ('translation task improving', 0.5647), ('translation tasks', 0.5586), ('machine translation', 0.5223), ('translation task', 0.4846)]","[('attention', 0.4124), ('decoder', 0.3694), ('parsing', 0.3525), ('encoder', 0.3448), ('neural', 0.3416)]","[('translation tasks', 0.5639), ('machine translation', 0.5361), ('translation task', 0.4974), ('attention', 0.4124), ('constituency parsing', 0.4004)]","[('machine translation tasks', 0.6226), ('translation task improving', 0.5762), ('translation tasks', 0.5639), ('machine translation', 0.5361), ('german translation task', 0.5046)]","[('attention', 0.8273)]","[('machine translation tasks', 0.6133), ('german translation task', 0.4838), ('french translation task', 0.4713), ('decoder', 0.3826), ('encoder', 0.3672)]","[('machine translation tasks', 0.6226), ('german translation task', 0.5046), ('french translation task', 0.4846), ('attention', 0.4124), ('decoder', 0.3694)]"
